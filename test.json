{"text": " I'm excited that you do's a new few extraction capability. Few extraction enables you to directly extract structured data from documents. For example, if you have a huge set of invoices and want to extract the date, vendingame, and items listed in every invoice, then a genetic document extraction now lets you clearly specify what data you want to extract using something called a schema, and then try to consistently extract those fields for you. Here's a sample invoice. I'm uploading this invoice to the agentate document extraction interface which processes the document and then presents some suggestions for fields extract, like invoice number, date, and vending details. The specification of what fields extract, the thing here is called a schema. For example, it says that the billing name is a string, and the line items are captured in an array, and so on, and you can edit a schema if you wish to suit your application. But I'm just going to accept the suggested schema and run it against the document. A few seconds later it has extracted the fields. For example, the invoice number, and the invoice total, and here it also has the list of all the items in this invoice. If you want to double check one of the answers, you can click on it, like I'm doing here, for the total amount that is 40,066,38, and the user interface also shows the visual grounding, meaning in the case where the document is extracted at few. I'm demonstrating this in the web user interface. For developers, the schema gets compiled to a set of pidantic clauses which is a common tool for validating data. You can see the code here. And when you run this in code, you paste the pidantic clause definitions into your code, which I've done here in the Jupyter notebook. Then using the agentate document extraction Python library, with just a few lines of code like this shown here, you can extract the fields from an invoice and again, you can get all the fields extracted. A few extraction is useful for any repetitive document processing, such as if you have a large collection of patient data records and wants to extract out the patient name, patient ID, diagnostics, and so on from each record. Or if you have a large collection of car insurance claim forms and want to extract out the data, the incident, vehicle information, and insurance information. Many businesses and individuals have huge collections of forms save somewhere. Fear the extraction less you quickly extract out the key fields from these documents for batch processing. I hope you find this useful and I look forward to seeing what you build of it.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.68, "text": " I'm excited that you do's a new few extraction capability.", "tokens": [50364, 286, 478, 2919, 300, 291, 360, 311, 257, 777, 1326, 30197, 13759, 13, 50548], "temperature": 0, "avg_logprob": -0.26255448659261066, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.007098410278558731}, {"id": 1, "seek": 0, "start": 3.68, "end": 9.52, "text": " Few extraction enables you to directly extract structured data from documents.", "tokens": [50548, 33468, 30197, 17077, 291, 281, 3838, 8947, 18519, 1412, 490, 8512, 13, 50840], "temperature": 0, "avg_logprob": -0.26255448659261066, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.007098410278558731}, {"id": 2, "seek": 0, "start": 9.52, "end": 14.96, "text": " For example, if you have a huge set of invoices and want to extract the date,", "tokens": [50840, 1171, 1365, 11, 498, 291, 362, 257, 2603, 992, 295, 1048, 78, 1473, 293, 528, 281, 8947, 264, 4002, 11, 51112], "temperature": 0, "avg_logprob": -0.26255448659261066, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.007098410278558731}, {"id": 3, "seek": 0, "start": 14.96, "end": 20.56, "text": " vendingame, and items listed in every invoice, then a genetic document extraction", "tokens": [51112, 371, 2029, 529, 11, 293, 4754, 10052, 294, 633, 47919, 11, 550, 257, 12462, 4166, 30197, 51392], "temperature": 0, "avg_logprob": -0.26255448659261066, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.007098410278558731}, {"id": 4, "seek": 0, "start": 20.56, "end": 26.96, "text": " now lets you clearly specify what data you want to extract using something called a schema,", "tokens": [51392, 586, 6653, 291, 4448, 16500, 437, 1412, 291, 528, 281, 8947, 1228, 746, 1219, 257, 34078, 11, 51712], "temperature": 0, "avg_logprob": -0.26255448659261066, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.007098410278558731}, {"id": 5, "seek": 2696, "start": 27.04, "end": 30.560000000000002, "text": " and then try to consistently extract those fields for you.", "tokens": [50368, 293, 550, 853, 281, 14961, 8947, 729, 7909, 337, 291, 13, 50544], "temperature": 0, "avg_logprob": -0.2866443872451782, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.00018499340512789786}, {"id": 6, "seek": 2696, "start": 30.560000000000002, "end": 36.64, "text": " Here's a sample invoice. I'm uploading this invoice to the agentate document extraction", "tokens": [50544, 1692, 311, 257, 6889, 47919, 13, 286, 478, 27301, 341, 47919, 281, 264, 9461, 473, 4166, 30197, 50848], "temperature": 0, "avg_logprob": -0.2866443872451782, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.00018499340512789786}, {"id": 7, "seek": 2696, "start": 36.64, "end": 42.72, "text": " interface which processes the document and then presents some suggestions for fields extract,", "tokens": [50848, 9226, 597, 7555, 264, 4166, 293, 550, 13533, 512, 13396, 337, 7909, 8947, 11, 51152], "temperature": 0, "avg_logprob": -0.2866443872451782, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.00018499340512789786}, {"id": 8, "seek": 2696, "start": 42.72, "end": 46.64, "text": " like invoice number, date, and vending details.", "tokens": [51152, 411, 47919, 1230, 11, 4002, 11, 293, 371, 2029, 4365, 13, 51348], "temperature": 0, "avg_logprob": -0.2866443872451782, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.00018499340512789786}, {"id": 9, "seek": 2696, "start": 47.6, "end": 52.64, "text": " The specification of what fields extract, the thing here is called a schema.", "tokens": [51396, 440, 31256, 295, 437, 7909, 8947, 11, 264, 551, 510, 307, 1219, 257, 34078, 13, 51648], "temperature": 0, "avg_logprob": -0.2866443872451782, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.00018499340512789786}, {"id": 10, "seek": 5264, "start": 53.52, "end": 60.4, "text": " For example, it says that the billing name is a string, and the line items are captured in an array,", "tokens": [50408, 1171, 1365, 11, 309, 1619, 300, 264, 35618, 1315, 307, 257, 6798, 11, 293, 264, 1622, 4754, 366, 11828, 294, 364, 10225, 11, 50752], "temperature": 0, "avg_logprob": -0.19491568172679227, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.0022850066889077425}, {"id": 11, "seek": 5264, "start": 60.96, "end": 65.76, "text": " and so on, and you can edit a schema if you wish to suit your application.", "tokens": [50780, 293, 370, 322, 11, 293, 291, 393, 8129, 257, 34078, 498, 291, 3172, 281, 5722, 428, 3861, 13, 51020], "temperature": 0, "avg_logprob": -0.19491568172679227, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.0022850066889077425}, {"id": 12, "seek": 5264, "start": 66.88, "end": 72.0, "text": " But I'm just going to accept the suggested schema and run it against the document.", "tokens": [51076, 583, 286, 478, 445, 516, 281, 3241, 264, 10945, 34078, 293, 1190, 309, 1970, 264, 4166, 13, 51332], "temperature": 0, "avg_logprob": -0.19491568172679227, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.0022850066889077425}, {"id": 13, "seek": 5264, "start": 72.0, "end": 77.44, "text": " A few seconds later it has extracted the fields. For example, the invoice number,", "tokens": [51332, 316, 1326, 3949, 1780, 309, 575, 34086, 264, 7909, 13, 1171, 1365, 11, 264, 47919, 1230, 11, 51604], "temperature": 0, "avg_logprob": -0.19491568172679227, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.0022850066889077425}, {"id": 14, "seek": 7744, "start": 78.32, "end": 85.44, "text": " and the invoice total, and here it also has the list of all the items in this invoice.", "tokens": [50408, 293, 264, 47919, 3217, 11, 293, 510, 309, 611, 575, 264, 1329, 295, 439, 264, 4754, 294, 341, 47919, 13, 50764], "temperature": 0, "avg_logprob": -0.27102591774680396, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.0009243942331522703}, {"id": 15, "seek": 7744, "start": 87.2, "end": 91.6, "text": " If you want to double check one of the answers, you can click on it, like I'm doing here,", "tokens": [50852, 759, 291, 528, 281, 3834, 1520, 472, 295, 264, 6338, 11, 291, 393, 2052, 322, 309, 11, 411, 286, 478, 884, 510, 11, 51072], "temperature": 0, "avg_logprob": -0.27102591774680396, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.0009243942331522703}, {"id": 16, "seek": 7744, "start": 92.24, "end": 100.24, "text": " for the total amount that is 40,066,38, and the user interface also shows the visual grounding,", "tokens": [51104, 337, 264, 3217, 2372, 300, 307, 3356, 11, 12791, 21, 11, 12625, 11, 293, 264, 4195, 9226, 611, 3110, 264, 5056, 46727, 11, 51504], "temperature": 0, "avg_logprob": -0.27102591774680396, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.0009243942331522703}, {"id": 17, "seek": 7744, "start": 100.24, "end": 103.75999999999999, "text": " meaning in the case where the document is extracted at few.", "tokens": [51504, 3620, 294, 264, 1389, 689, 264, 4166, 307, 34086, 412, 1326, 13, 51680], "temperature": 0, "avg_logprob": -0.27102591774680396, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.0009243942331522703}, {"id": 18, "seek": 10376, "start": 104.08, "end": 111.36, "text": " I'm demonstrating this in the web user interface. For developers, the schema gets", "tokens": [50380, 286, 478, 29889, 341, 294, 264, 3670, 4195, 9226, 13, 1171, 8849, 11, 264, 34078, 2170, 50744], "temperature": 0, "avg_logprob": -0.265624205271403, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.0010809155646711588}, {"id": 19, "seek": 10376, "start": 111.36, "end": 118.16000000000001, "text": " compiled to a set of pidantic clauses which is a common tool for validating data.", "tokens": [50744, 36548, 281, 257, 992, 295, 280, 327, 7128, 49072, 597, 307, 257, 2689, 2290, 337, 7363, 990, 1412, 13, 51084], "temperature": 0, "avg_logprob": -0.265624205271403, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.0010809155646711588}, {"id": 20, "seek": 10376, "start": 118.80000000000001, "end": 120.56, "text": " You can see the code here.", "tokens": [51116, 509, 393, 536, 264, 3089, 510, 13, 51204], "temperature": 0, "avg_logprob": -0.265624205271403, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.0010809155646711588}, {"id": 21, "seek": 10376, "start": 124.0, "end": 130.72, "text": " And when you run this in code, you paste the pidantic clause definitions into your code,", "tokens": [51376, 400, 562, 291, 1190, 341, 294, 3089, 11, 291, 9163, 264, 280, 327, 7128, 25925, 21988, 666, 428, 3089, 11, 51712], "temperature": 0, "avg_logprob": -0.265624205271403, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.0010809155646711588}, {"id": 22, "seek": 13072, "start": 131.2, "end": 133.6, "text": " which I've done here in the Jupyter notebook.", "tokens": [50388, 597, 286, 600, 1096, 510, 294, 264, 22125, 88, 391, 21060, 13, 50508], "temperature": 0, "avg_logprob": -0.22340987858019376, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0003350168699398637}, {"id": 23, "seek": 13072, "start": 134.48, "end": 138.96, "text": " Then using the agentate document extraction Python library,", "tokens": [50552, 1396, 1228, 264, 9461, 473, 4166, 30197, 15329, 6405, 11, 50776], "temperature": 0, "avg_logprob": -0.22340987858019376, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0003350168699398637}, {"id": 24, "seek": 13072, "start": 139.6, "end": 146.24, "text": " with just a few lines of code like this shown here, you can extract the fields from an invoice", "tokens": [50808, 365, 445, 257, 1326, 3876, 295, 3089, 411, 341, 4898, 510, 11, 291, 393, 8947, 264, 7909, 490, 364, 47919, 51140], "temperature": 0, "avg_logprob": -0.22340987858019376, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0003350168699398637}, {"id": 25, "seek": 13072, "start": 146.24, "end": 149.44, "text": " and again, you can get all the fields extracted.", "tokens": [51140, 293, 797, 11, 291, 393, 483, 439, 264, 7909, 34086, 13, 51300], "temperature": 0, "avg_logprob": -0.22340987858019376, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0003350168699398637}, {"id": 26, "seek": 13072, "start": 151.44, "end": 155.76, "text": " A few extraction is useful for any repetitive document processing,", "tokens": [51400, 316, 1326, 30197, 307, 4420, 337, 604, 29404, 4166, 9007, 11, 51616], "temperature": 0, "avg_logprob": -0.22340987858019376, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0003350168699398637}, {"id": 27, "seek": 15576, "start": 155.76, "end": 161.12, "text": " such as if you have a large collection of patient data records and wants to extract out the", "tokens": [50364, 1270, 382, 498, 291, 362, 257, 2416, 5765, 295, 4537, 1412, 7724, 293, 2738, 281, 8947, 484, 264, 50632], "temperature": 0, "avg_logprob": -0.18854741616682572, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00014412161544896662}, {"id": 28, "seek": 15576, "start": 161.12, "end": 164.88, "text": " patient name, patient ID, diagnostics, and so on from each record.", "tokens": [50632, 4537, 1315, 11, 4537, 7348, 11, 43215, 1167, 11, 293, 370, 322, 490, 1184, 2136, 13, 50820], "temperature": 0, "avg_logprob": -0.18854741616682572, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00014412161544896662}, {"id": 29, "seek": 15576, "start": 165.44, "end": 171.04, "text": " Or if you have a large collection of car insurance claim forms and want to extract out the data,", "tokens": [50848, 1610, 498, 291, 362, 257, 2416, 5765, 295, 1032, 7214, 3932, 6422, 293, 528, 281, 8947, 484, 264, 1412, 11, 51128], "temperature": 0, "avg_logprob": -0.18854741616682572, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00014412161544896662}, {"id": 30, "seek": 15576, "start": 171.04, "end": 174.39999999999998, "text": " the incident, vehicle information, and insurance information.", "tokens": [51128, 264, 9348, 11, 5864, 1589, 11, 293, 7214, 1589, 13, 51296], "temperature": 0, "avg_logprob": -0.18854741616682572, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00014412161544896662}, {"id": 31, "seek": 15576, "start": 175.44, "end": 180.07999999999998, "text": " Many businesses and individuals have huge collections of forms save somewhere.", "tokens": [51348, 5126, 6011, 293, 5346, 362, 2603, 16641, 295, 6422, 3155, 4079, 13, 51580], "temperature": 0, "avg_logprob": -0.18854741616682572, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00014412161544896662}, {"id": 32, "seek": 18008, "start": 180.8, "end": 185.92000000000002, "text": " Fear the extraction less you quickly extract out the key fields from these documents for batch", "tokens": [50400, 28054, 264, 30197, 1570, 291, 2661, 8947, 484, 264, 2141, 7909, 490, 613, 8512, 337, 15245, 50656], "temperature": 0, "avg_logprob": -0.2895905331867497, "compression_ratio": 1.3909774436090225, "no_speech_prob": 0.0018323431722819805}, {"id": 33, "seek": 18008, "start": 185.92000000000002, "end": 190.64000000000001, "text": " processing. I hope you find this useful and I look forward to seeing what you build of it.", "tokens": [50656, 9007, 13, 286, 1454, 291, 915, 341, 4420, 293, 286, 574, 2128, 281, 2577, 437, 291, 1322, 295, 309, 13, 50892], "temperature": 0, "avg_logprob": -0.2895905331867497, "compression_ratio": 1.3909774436090225, "no_speech_prob": 0.0018323431722819805}], "language": "en"}